{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Emotion Recognition - wav2vec2-XLSR-53 large - Spanish**\n",
        "---\n",
        "\n",
        "***References***\n",
        "\n",
        "[Wav2Vec2 - Spanish](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-spanish)\n",
        "\n",
        "[Wav2Vec2 - Trasnfer learning](https://github.com/amansyayf/wav2vec2_emotion_recognition/blob/main/training_module.ipynb)\n"
      ],
      "metadata": {
        "id": "GM_6ltN0pb3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preparation**\n",
        "---"
      ],
      "metadata": {
        "id": "p6rBnyNXqewR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcq49Agapair"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Path de los archivos zip\n",
        "mesd_zip =  '/content/drive/MyDrive/MESD.zip'\n",
        "ems_zip  =  '/content/drive/MyDrive/EmoMatchSpanishDB/EmoMatchSpanishDB.zip'\n",
        "smc_zip =   '/content/drive/MyDrive/Spanish MeaCorpus/sp.zip'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile(mesd_zip, 'r')\n",
        "zip_ref.extractall('/content/mesd')\n",
        "zip_ref.close()\n",
        "!ls"
      ],
      "metadata": {
        "id": "UZV_P08wqfXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([mesd_df, smc_df, ems_df], ignore_index=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Wgdumu26qfaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([mesd_df, smc_df], ignore_index=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "K5lIeOCRqfch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.label.unique()"
      ],
      "metadata": {
        "id": "QA2GxLPeqffC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    'Anger': 'Ira', 'anger': 'Ira',\n",
        "    'Disgust': 'Asco', 'disgust': 'Asco',\n",
        "    'Fear': 'Miedo', 'fear': 'Miedo',\n",
        "    'Happiness': 'Alegria', 'joy': 'Alegria', 'happiness': 'Alegria',\n",
        "    'Neutral': 'Neutro', 'neutral': 'Neutro',\n",
        "    'Sadness': 'Tristeza', 'sadness': 'Tristeza',\n",
        "    'Alegria': 'Alegria',\n",
        "    'Asco': 'Asco',\n",
        "    'Ira': 'Ira',\n",
        "    'Miedo': 'Miedo',\n",
        "    'Neutro': 'Neutro',\n",
        "    'Sorpresa': 'Sorpresa',\n",
        "    'Tristeza': 'Tristeza'\n",
        "}\n",
        "\n",
        "df['label'] = df['label'].map(label_mapping)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7EwpDRToqfhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n",
        "df_val, df_test = train_test_split(df_val, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "YE0JbXJop02O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_emotions = len(df['label_class'].unique())"
      ],
      "metadata": {
        "id": "tZdPn18cp04E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set-up training**\n",
        "---"
      ],
      "metadata": {
        "id": "oO27-cBqqAzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "8Eo9ANo-p0Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, df, data_col, label_col, max_length=10*16000, new_sr=16000):\n",
        "\n",
        "        self.file_path_list = df[data_col].tolist()\n",
        "        self.label_list = df[label_col].tolist()\n",
        "        self.max_length = max_length\n",
        "        self.new_sr = new_sr\n",
        "\n",
        "        total_len = len(self.file_path_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_path_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio, sample_rate = librosa.load(self.file_path_list[idx])\n",
        "        if sample_rate != self.new_sr:\n",
        "            audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=self.new_sr)\n",
        "        label = self.label_list[idx]\n",
        "\n",
        "        desired_length = self.max_length\n",
        "\n",
        "        # pad or trim the audio signal to the desired length\n",
        "        # pad the audio tensor with zeros to a fixed length of 160000\n",
        "        if len(audio) < desired_length:\n",
        "            padding = desired_length - len(audio)\n",
        "            audio = np.pad(audio, (0, padding), 'constant')\n",
        "        elif len(audio) > desired_length:\n",
        "            audio = audio[:desired_length]\n",
        "        return audio, label, self.file_path_list[idx]"
      ],
      "metadata": {
        "id": "DFmRYsRpp06j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "\n",
        "def get_dataloaders(df_train, df_val, df_test, BATCH_SIZE=8):\n",
        "\n",
        "  train_dataset = AudioDataset(df_train, 'path', 'label_class')\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "  val_dataset = AudioDataset(df_val, 'path', 'label_class')\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "  test_dataset = AudioDataset(df_test, 'path', 'label_class')\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "  dataloaders = {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}\n",
        "\n",
        "  return dataloaders"
      ],
      "metadata": {
        "id": "ACNeiz0Mp09D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import HubertModel, Wav2Vec2FeatureExtractor, Wav2Vec2Tokenizer, Wav2Vec2Model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self,path):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(path)\n",
        "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(path)\n",
        "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
        "\n",
        "        # Ajusta las capas convolucionales según sea necesario\n",
        "        self.conv1 = nn.Conv1d(499, 256, 1)\n",
        "        self.dropout1 = torch.nn.Dropout(0.5)\n",
        "        self.conv2 = nn.Conv1d(256, 1, 1)\n",
        "        self.fc1 = torch.nn.Linear(1024,256)  # Ajusta la entrada según la nueva longitud de la secuencia\n",
        "        self.dropout2 = torch.nn.Dropout(0.5)\n",
        "        self.fc2 = torch.nn.Linear(256, num_emotions)\n",
        "\n",
        "\n",
        "    def forward(self, input, spec_aug=False, mixup_lambda=None):\n",
        "        input = self.feature_extractor(input, return_tensors=\"pt\", sampling_rate=16000).to(device)\n",
        "        input = input.input_values.squeeze(dim=0) # shape = (Batch_size, 16000)\n",
        "        wav2feature = self.wav2vec2(input).last_hidden_state  # shape = (498, 768)\n",
        "\n",
        "        # wav2feature = torch.mean(wav2feature, dim=1)\n",
        "        # wav2feature = wav2feature.permute(0, 2, 1)  # Cambia a (batch_size, 768, sequence_length) -> (batch_size, 768, 498)\n",
        "\n",
        "        x = self.dropout1(F.relu(self.conv1(wav2feature))) # shape = (N , 468, 501)\n",
        "        x = self.conv2(x) # shape = (N, 501, 468)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.dropout2(F.relu(self.fc1(x))) # shape = (N,468)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.nn.functional.softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "f_yDA0Dsp0_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer' : optimizer.state_dict()}\n",
        "    torch.save(state, checkpoint_path)\n",
        "    print('model saved to %s' % checkpoint_path)\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "    optimizer.load_state_dict(state['optimizer'])\n",
        "    print('model loaded from %s' % checkpoint_path)\n"
      ],
      "metadata": {
        "id": "Fpf3m_oDp1B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def compute_metrics(y_true, y_pred, num_classes):\n",
        "    # Calculando la matriz de confusión\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
        "\n",
        "    # Calculando precision, recall y f1-score\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, labels=list(range(num_classes)), average=None)\n",
        "\n",
        "    # Calculando métricas por clase\n",
        "    class_accuracy = np.diag(cm) / np.sum(cm, axis=1)\n",
        "\n",
        "    return {\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"fscore\": fscore,\n",
        "        \"class_accuracy\": class_accuracy,\n",
        "    }\n",
        "\n",
        "class Learner():\n",
        "  def __init__(self, model, opt, dataloaders, loss_fn, device, checkpoint_path):\n",
        "    self.model = model\n",
        "    self.opt = opt\n",
        "    self.data_loader = dataloaders\n",
        "    self.loss_fn = loss_fn\n",
        "    self.device = device\n",
        "    self.checkpoint_path = checkpoint_path\n",
        "    self.scaler = GradScaler()\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    state = {\n",
        "        'state_dict': self.model.state_dict(),\n",
        "        'optimizer' : self.opt.state_dict()}\n",
        "    torch.save(state, self.checkpoint_path)\n",
        "    print('model saved to %s' % self.checkpoint_path)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "      state = torch.load(self.checkpoint_path)\n",
        "      self.model.load_state_dict(state['state_dict'])\n",
        "      self.opt.load_state_dict(state['optimizer'])\n",
        "      print('model loaded from %s' % self.checkpoint_path)\n",
        "\n",
        "  def accuracy_fn(self, y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred))\n",
        "    return acc\n",
        "\n",
        "  def train_step(self, train_losses = [], train_accuracies=[]):\n",
        "      train_loss, train_acc = 0, 0\n",
        "      self.model.train()\n",
        "      accumulation_steps = 4  # Número de pasos de acumulación\n",
        "\n",
        "      for batch, (X, y, file_path) in enumerate(tqdm(self.data_loader['train'], desc=\"Training\", leave=False)):\n",
        "        X, y = X.to(self.device), y.to(self.device)\n",
        "        with autocast():\n",
        "            y_prob = self.model(X)\n",
        "            loss = self.loss_fn(torch.log(y_prob), y)\n",
        "            loss = loss / accumulation_steps  # Escala la pérdida\n",
        "        self.scaler.scale(loss).backward()\n",
        "\n",
        "        if (batch + 1) % accumulation_steps == 0:\n",
        "            self.scaler.step(self.opt)\n",
        "            self.scaler.update()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "        y_pred = torch.argmax(y_prob, dim=1)\n",
        "        train_loss += loss.item() * accumulation_steps\n",
        "        acc = self.accuracy_fn(y_true=y, y_pred=y_pred)\n",
        "        train_acc += acc\n",
        "\n",
        "      train_loss /= len(self.data_loader['train'])\n",
        "      train_acc /= len(self.data_loader['train'])\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      train_accuracies.append(train_acc)\n",
        "\n",
        "  def val_step(self, val_losses = [], val_accuracies = [], key='val'):\n",
        "      y_trues, y_preds = [], []\n",
        "      val_loss, val_acc = 0, 0\n",
        "\n",
        "      self.model.eval()\n",
        "      with torch.no_grad():\n",
        "          for batch, (X, y, file_path) in enumerate(tqdm(self.data_loader[key], desc=\"Validating\", leave=False)):\n",
        "              X, y = X.to(self.device), y.to(self.device)\n",
        "              with autocast():\n",
        "                  val_prob = self.model(X)\n",
        "                  val_pred = torch.argmax(val_prob, dim=1)\n",
        "                  loss = self.loss_fn(torch.log(val_prob), y)\n",
        "              val_loss += loss.item()\n",
        "              acc = self.accuracy_fn(y_true=y, y_pred=val_pred)\n",
        "              val_acc += acc\n",
        "              y_trues.extend(y.cpu().numpy())\n",
        "              y_preds.extend(val_pred.cpu().numpy())\n",
        "\n",
        "          val_loss /= len(self.data_loader[key])\n",
        "          val_acc /= len(self.data_loader[key])\n",
        "\n",
        "          metrics = compute_metrics(np.array(y_trues), np.array(y_preds), num_classes=num_emotions)\n",
        "\n",
        "          if key == 'val':\n",
        "              if val_accuracies and val_acc > max(val_accuracies):\n",
        "                  self.save_checkpoint()\n",
        "\n",
        "              val_losses.append(val_loss)\n",
        "              val_accuracies.append(val_acc)\n",
        "\n",
        "          if key == 'test':\n",
        "              return {\"model_loss\": val_loss, \"model_acc\": val_acc, \"metrics\": metrics}\n",
        "\n",
        "  def test(self):\n",
        "      if os.path.isfile(self.checkpoint_path):\n",
        "          self.load_checkpoint()\n",
        "      return self.val_step(key = 'test')\n",
        "\n",
        "  def fit(self, epochs = 15):\n",
        "      train_losses, val_losses = [], []\n",
        "      train_accuracies, val_accuracies = [], []\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          self.train_step(train_losses = train_losses, train_accuracies =train_accuracies)\n",
        "          self.val_step(val_losses = val_losses, val_accuracies = val_accuracies, key = 'val')\n",
        "\n",
        "          clear_output(True)\n",
        "\n",
        "          fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "          axes[0].set_title('Loss')\n",
        "          axes[0].plot(train_losses, label='train')\n",
        "          axes[0].plot(val_losses, label='val')\n",
        "          axes[0].legend(loc='upper right')\n",
        "          axes[0].grid()\n",
        "\n",
        "          axes[1].set_title('Accuracy')\n",
        "          axes[1].plot(train_accuracies, label='train')\n",
        "          axes[1].plot(val_accuracies, label='val')\n",
        "          axes[1].legend(loc='upper right')\n",
        "          axes[1].grid()\n",
        "\n",
        "          plt.show()\n",
        "\n",
        "          print(f\"Epoch {epoch} || train_loss: {train_losses[-1]}, val_loss: {val_losses[-1]}, train_accuracy: {train_accuracies[-1]}, val_accuracy: {val_accuracies[-1]}\")"
      ],
      "metadata": {
        "id": "q1Lm6acCqFJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**\n",
        "---"
      ],
      "metadata": {
        "id": "bMcrxFNArMFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=8\n",
        "\n",
        "dataloaders = get_dataloaders(df_train, df_val, df_test, BATCH_SIZE=BATCH_SIZE)\n",
        "\n",
        "model = AudioClassifier(\"facebook/wav2vec2-large-xlsr-53-spanish\").to(device)\n",
        "# next(model.parameters()).device\n",
        "\n",
        "loss_fn = nn.NLLLoss() # Multi-category loss\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.00005, betas=(0.5, 0.9))\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/wav2vec/ser.pth'"
      ],
      "metadata": {
        "id": "xF8NajjiqFMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learner =  Learner(model, optimizer, dataloaders, loss_fn, device, checkpoint_path = checkpoint_path)\n",
        "learner.fit(epochs = 15)"
      ],
      "metadata": {
        "id": "xKL-dr0IqFO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learner.test()"
      ],
      "metadata": {
        "id": "jQxDaWMQqbHx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}